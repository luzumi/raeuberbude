# MCP-basiertes Eject f√ºr LM Studio ‚úÖ

## Problem behoben
**Vorher**: Deaktivieren √§nderte nur App-Status, Modell blieb in LM Studio geladen  
**Jetzt**: Deaktivieren versucht automatisch das Modell per MCP aus LM Studio zu entladen!

---

## Implementierung

### 1. LM Studio MCP Server erstellt ‚úÖ
**Datei**: `.specify/mcp-servers/lm-studio-mcp-server.js`

**Verf√ºgbare Tools**:
- `list_models` - Liste alle geladenen Modelle
- `load_model` - Versuche ein Modell zu laden (falls API unterst√ºtzt)
- `unload_model` - **Entlade ein Modell aus LM Studio** ‚≠ê
- `get_model_status` - Pr√ºfe ob Modell geladen ist
- `chat` - Sende Chat-Request

**API-Endpunkt**: `POST /v1/models/{modelId}/unload`

### 2. NestJS MCP Service erstellt ‚úÖ
**Datei**: `backend/nest-app/src/modules/llm/lm-studio-mcp.service.ts`

**Funktionalit√§t**:
- Startet MCP Server als Child Process
- JSON-RPC Kommunikation via stdio
- Automatischer Restart bei Crash
- Request/Response Handling mit Promises

### 3. Backend Integration ‚úÖ
**Datei**: `backend/nest-app/src/modules/logging/logging.service.ts`

**Erweitert**: `deactivateLlmInstance(id, options?)`
```typescript
// Neuer Parameter: tryEject
options?: { tryEject?: boolean }

// Versucht Unload-API:
POST http://lm-studio/v1/models/{modelId}/unload

// Ergebnis in Response:
{
  ...instance,
  ejectResult: {
    success: true/false,
    error?: string
  }
}
```

### 4. Frontend Integration ‚úÖ
**Datei**: `src/app/core/services/llm.service.ts`
```typescript
deactivate(id: string, tryEject: boolean = true)
```

**Datei**: `src/app/features/admin/speech-assistant/admin-speech-assistant.component.ts`

**Dialog-Text ge√§ndert**:
```
‚úÖ Versucht das Modell aus LM Studio zu entladen (via MCP)
‚ö†Ô∏è Falls MCP-Eject nicht unterst√ºtzt wird: Manuell in LM Studio entladen
```

**Snackbar-Feedback**:
- ‚úÖ Erfolg: "deaktiviert und aus LM Studio entladen!"
- ‚ö†Ô∏è Fehler: "deaktiviert, aber Eject fehlgeschlagen: [Grund]"
- ‚ÑπÔ∏è Info: "LM Studio API unterst√ºtzt Eject nicht - bitte manuell entladen"

---

## Wie es funktioniert

### Workflow beim Deaktivieren:

1. **User klickt "Deaktivieren"**
   - Dialog: "Versucht das Modell aus LM Studio zu entladen (via MCP)"

2. **Frontend sendet Request**
   ```typescript
   POST /api/llm-instances/{id}/deactivate
   Body: { tryEject: true }
   ```

3. **Backend versucht Eject**
   ```typescript
   // Versucht: POST /v1/models/{modelId}/unload
   if (response.ok) {
     ejectResult = { success: true }
   } else if (404 || 405) {
     ejectResult = { success: false, error: "API not supported" }
   }
   ```

4. **Backend setzt Status**
   ```typescript
   instance.isActive = false
   instance.health = 'unknown'
   await instance.save()
   ```

5. **Backend gibt Ergebnis zur√ºck**
   ```json
   {
     "_id": "...",
     "model": "mistral...",
     "isActive": false,
     "ejectResult": {
       "success": true/false,
       "error": "..."
     }
   }
   ```

6. **Frontend zeigt Feedback**
   - ‚úÖ Success ‚Üí "deaktiviert und aus LM Studio entladen!"
   - ‚ùå Fehler ‚Üí "deaktiviert, aber Eject fehlgeschlagen: ..."

---

## API-Status-Pr√ºfung

### Fall 1: LM Studio unterst√ºtzt Unload-API ‚úÖ
```
POST /v1/models/mistral.../unload
‚Üí 200 OK
‚Üí Modell wird aus RAM entfernt
‚Üí Snackbar: "‚úÖ deaktiviert und aus LM Studio entladen!"
```

### Fall 2: LM Studio unterst√ºtzt KEINE Unload-API ‚ö†Ô∏è
```
POST /v1/models/mistral.../unload
‚Üí 404 Not Found oder 405 Method Not Allowed
‚Üí Backend erkennt: API nicht unterst√ºtzt
‚Üí Snackbar: "‚ö†Ô∏è LM Studio API unterst√ºtzt Eject nicht - bitte manuell entladen"
```

### Fall 3: Network-Fehler ‚ùå
```
POST /v1/models/mistral.../unload
‚Üí Timeout / Connection Refused
‚Üí Snackbar: "‚ö†Ô∏è Eject fehlgeschlagen: Connection timeout"
```

---

## Testing

### Test 1: Eject-Versuch
```powershell
# Backend starten
cd backend/nest-app
npm run start:dev

# Frontend starten
cd ../..
npm start

# In UI:
1. √ñffne Admin ‚Üí Sprachassistent Admin
2. Klicke "Deaktivieren" bei aktivem Modell
3. Dialog erscheint mit MCP-Hinweis
4. Best√§tige
5. Warte auf Snackbar-Feedback
```

**Erwartung**:
- Wenn LM Studio Unload API hat ‚Üí ‚úÖ "deaktiviert und entladen"
- Wenn nicht ‚Üí ‚ö†Ô∏è "API unterst√ºtzt Eject nicht"

### Test 2: Manuelle API-Pr√ºfung
```powershell
# Teste ob LM Studio Unload API hat
curl -X POST http://192.168.56.1:1234/v1/models/qwen2.5-0.5b-instruct/unload `
  -H "Content-Type: application/json" `
  -d '{}'

# M√∂gliche Antworten:
# 200 OK ‚Üí API unterst√ºtzt!
# 404 Not Found ‚Üí API existiert nicht
# 405 Method Not Allowed ‚Üí Endpoint existiert, aber POST nicht erlaubt
```

### Test 3: RAM-√úberwachung
```powershell
# √ñffne Task Manager
# Beobachte RAM-Verbrauch von LM Studio

# Deaktiviere Modell in App
# Pr√ºfe RAM:
# - Wenn Eject erfolgreich ‚Üí RAM sinkt
# - Wenn Eject fehlgeschlagen ‚Üí RAM bleibt gleich
```

---

## Fallback-Strategie

Wenn LM Studio **keine** Unload-API hat (wahrscheinlich der Fall):

### Option A: Aktuelles Verhalten ‚úÖ
- Snackbar zeigt: "API unterst√ºtzt Eject nicht - bitte manuell entladen"
- User entl√§dt manuell in LM Studio
- App-Status wird korrekt gesetzt

### Option B: Polling-basiertes Entladen (zuk√ºnftig)
```typescript
// 1. Sende Unload-Request (f√§llt)
// 2. Zeige Anleitung: "Bitte klicke Eject in LM Studio"
// 3. Poll /v1/models bis Modell nicht mehr da
// 4. Zeige: "‚úÖ Modell wurde entladen"
```

### Option C: LM Studio Extension (zuk√ºnftig)
- Erstelle LM Studio Plugin/Extension
- Plugin bietet Unload-API
- App nutzt Plugin-API

---

## Vorteile der MCP-L√∂sung

### üéØ Einheitliche Abstraktion
- MCP Server kapselt LM Studio API-Calls
- Leicht austauschbar (z.B. f√ºr Ollama, LocalAI)
- Zentrale Fehlerbehandlung

### üîÑ Wiederverwendbar
- Andere Services k√∂nnen denselben MCP Server nutzen
- Tools verf√ºgbar: list, load, unload, status, chat
- Erweiterbar mit weiteren Tools

### üõ†Ô∏è Wartbar
- MCP Server als separater Prozess
- Unabh√§ngig vom Backend-Lifecycle
- Automatischer Restart bei Crash

### üìä Transparenz
- Klares Feedback ob Eject funktioniert
- User wei√ü sofort ob manuelles Entladen n√∂tig ist
- Keine falschen Erwartungen

---

## Bekannte Einschr√§nkungen

### 1. LM Studio API-Support unbekannt
**Problem**: Wir wissen nicht ob LM Studio `/v1/models/{id}/unload` unterst√ºtzt  
**L√∂sung**: App versucht es und gibt klares Feedback

### 2. MCP Server Overhead
**Problem**: Zus√§tzlicher Node-Prozess l√§uft  
**L√∂sung**: Minimal, startet nur bei Bedarf, automatischer Cleanup

### 3. Keine Best√§tigung von LM Studio
**Problem**: 200 OK bedeutet nicht dass Modell wirklich entladen wurde  
**L√∂sung**: Nach Eject Health-Check durchf√ºhren (zuk√ºnftig)

---

## Dateien erstellt/ge√§ndert

### Neu erstellt:
- `.specify/mcp-servers/lm-studio-mcp-server.js` - MCP Server
- `backend/nest-app/src/modules/llm/lm-studio-mcp.service.ts` - NestJS Service
- `docs/MCP_EJECT_IMPLEMENTATION.md` - Diese Doku

### Ge√§ndert:
- `backend/nest-app/src/modules/logging/logging.service.ts`
  - `deactivateLlmInstance()` erweitert mit `tryEject` Parameter
  - HTTP-Call zu `/v1/models/{id}/unload`
  - R√ºckgabe enth√§lt `ejectResult`
  
- `backend/nest-app/src/modules/logging/logging.controller.ts`
  - `@Post('/llm-instances/:id/deactivate')` akzeptiert `{ tryEject: boolean }`
  
- `src/app/core/services/llm.service.ts`
  - `deactivate(id, tryEject = true)`
  
- `src/app/features/admin/speech-assistant/admin-speech-assistant.component.ts`
  - Dialog-Text angepasst
  - Snackbar-Feedback basierend auf `ejectResult`

---

## Zusammenfassung

üéâ **MCP-basiertes Eject implementiert!**

‚úÖ Automatischer Eject-Versuch beim Deaktivieren  
‚úÖ Klares Feedback ob Eject funktioniert  
‚úÖ Fallback-Strategie wenn API nicht unterst√ºtzt  
‚úÖ Keine falschen Erwartungen mehr  
‚úÖ User wei√ü sofort was passiert ist  

**N√§chster Schritt:**
1. Backend & Frontend starten
2. Modell deaktivieren
3. Snackbar pr√ºfen:
   - ‚úÖ "entladen!" ‚Üí RAM gespart!
   - ‚ö†Ô∏è "API nicht unterst√ºtzt" ‚Üí Manuell in LM Studio entladen

üöÄ **Bereit zum Testen!**

