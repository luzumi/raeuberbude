# LLM Model Selection Feature

## √úbersicht

Die globalen LLM-Einstellungen wurden erweitert, um die Auswahl von Modellen aus LM Studio zu erm√∂glichen. Statt manueller Texteingabe k√∂nnen Benutzer nun aus einer Liste verf√ºgbarer Modelle w√§hlen.

## Implementierte Features

### 1. **Dropdown f√ºr Modellauswahl**
- Prim√§res Modell und Fallback-Modell werden jetzt als Dropdowns angezeigt
- Die Dropdowns werden erst aktiviert, nachdem Modelle von LM Studio geladen wurden

### 2. **Automatisches Laden der Modelle**
- Beim √ñffnen des Dialogs werden automatisch die verf√ºgbaren Modelle von der konfigurierten LM Studio URL geladen
- Ein "Refresh"-Button (üîÑ) neben dem URL-Feld erm√∂glicht das manuelle Neuladen der Modelle
- W√§hrend des Ladens wird ein Spinner angezeigt

### 3. **Fallback-Modell (optional)**
- Das Fallback-Modell kann leer gelassen werden (Option: "-- Kein Fallback --")
- N√ºtzlich, wenn kein Backup-Modell ben√∂tigt wird

### 4. **Slider f√ºr Parameter**
- **Temperature** (0.0 - 1.0): Steuert die Kreativit√§t des Modells
- **Max Tokens** (100 - 2000): Maximale Antwortl√§nge
- **Confidence Shortcut** (0.0 - 1.0): Schwellwert f√ºr Heuristik-Bypass
- **Timeout** (5s - 60s): Maximale Wartezeit f√ºr LLM-Antworten
- **CPU Threads** (1 - Kerne-1): Anzahl CPU-Threads f√ºr Inferenz

### 5. **Select-Dropdowns f√ºr erweiterte Einstellungen**
- **Context-L√§nge**: W√§hlbare Werte von 2K bis 32K Tokens
- **Batch-Gr√∂√üe**: Von 1 (einzeln) bis 32 f√ºr Batch-Verarbeitung

## Verwendung

### Schritt-f√ºr-Schritt Anleitung

1. **√ñffne die globalen LLM-Einstellungen:**
   - Navigiere zu Admin ‚Üí Speech Assistant
   - Klicke auf "Globale Einstellungen"

2. **LM Studio URL eingeben:**
   - Gib die URL deines LM Studio Servers ein (z.B. `http://192.168.56.1:1234`)
   - Die URL kann mit oder ohne `/v1/chat/completions` Pfad eingegeben werden

3. **Modelle laden:**
   - Die Modelle werden automatisch geladen, wenn eine URL vorhanden ist
   - Alternativ: Klicke auf den üîÑ Button neben dem URL-Feld
   - Warte, bis der Ladevorgang abgeschlossen ist

4. **Modelle ausw√§hlen:**
   - W√§hle aus dem Dropdown das prim√§re Modell
   - Optional: W√§hle ein Fallback-Modell

5. **Parameter anpassen:**
   - **Temperature**: H√∂here Werte (0.7-1.0) f√ºr kreativere Antworten, niedrigere (0.1-0.3) f√ºr pr√§zisere
   - **Max Tokens**: Anzahl der generierten Tokens (Standard: 500)
   - **Confidence Shortcut**: Bei hoher STT-Confidence wird LLM √ºbersprungen (Standard: 0.85)
   - **Timeout**: Maximale Wartezeit f√ºr LLM-Antworten (Standard: 30s)
   - **Context-L√§nge**: Speicher f√ºr vorherige Interaktionen (Standard: 4K)
   - **Batch-Gr√∂√üe**: Anzahl parallel verarbeiteter Anfragen (Standard: 1)
   - **CPU Threads**: Anzahl genutzter CPU-Kerne (wird automatisch auf Kerne-1 begrenzt)

6. **Einstellungen speichern:**
   - Klicke auf "Speichern"

## Technische Details

### Ge√§nderte Dateien

1. **admin-global-config-dialog.component.ts**
   - Hinzugef√ºgt: `LlmService` Injection
   - Hinzugef√ºgt: `OnInit` Interface
   - Hinzugef√ºgt: `availableModels` Array und `loadingModels` Flag
   - Hinzugef√ºgt: `loadModels()` Methode zum Abrufen der Modelle
   - Template aktualisiert: Textfelder durch `<mat-select>` ersetzt

2. **llm.service.ts**
   - Verbessert: `getModels()` Methode
   - Unterst√ºtzt jetzt verschiedene URL-Formate:
     - `http://host:port` ‚Üí versucht `/v1/models` und `/models`
     - `http://host:port/v1/chat/completions` ‚Üí ersetzt durch `/v1/models`
     - Automatische Normalisierung von URLs

3. **transcription-validator.service.ts**
   - Hinzugef√ºgt: `normalizeLMStudioUrl()` Methode
   - Stellt sicher, dass `/v1/chat/completions` am Ende der URL ist
   - Defensive Pr√ºfungen f√ºr LLM-Response-Struktur

### API Endpoints

Die `getModels()` Methode versucht folgende Endpoints in dieser Reihenfolge:

1. `{url}/v1/models` (Standard LM Studio Endpoint)
2. `{url}/models` (Alternative)
3. Bei Pfaden in der URL: `{origin}{path}/v1/models`

### Response Format

LM Studio `/v1/models` Endpoint gibt folgendes Format zur√ºck:

```json
{
  "object": "list",
  "data": [
    {
      "id": "mistralai/mistral-7b-instruct-v0.3",
      "object": "model",
      "owned_by": "organization-owner",
      "permission": []
    }
  ]
}
```

Die `getModels()` Methode extrahiert die `id`-Felder aus diesem Format.

## Bekannte Probleme & Fixes

### ‚úÖ Fixed: Select-Dropdowns werden beim Klicken deaktiviert
- **Problem:** Beim Klicken auf ein Select-Dropdown wurden die Dropdowns deaktiviert
- **Ursache:** Das `blur`-Event auf dem URL-Feld l√∂ste `onUrlChange()` aus, was die `availableModels` Liste leerte
- **Fix:** `blur`-Event und `onUrlChange()` Methode entfernt. Modelle werden jetzt nur manuell √ºber den üîÑ Button neu geladen

## Fehlerbehandlung

### Keine Modelle gefunden
- **Ursache:** LM Studio ist nicht erreichbar oder hat keine geladenen Modelle
- **L√∂sung:** 
  - Pr√ºfe, ob LM Studio l√§uft
  - Pr√ºfe die URL (korrekte IP und Port)
  - Lade mindestens ein Modell in LM Studio

### Fehler beim Laden der Modelle
- **Ursache:** Netzwerkfehler, falsche URL, CORS-Problem
- **L√∂sung:**
  - Pr√ºfe die Browser-Konsole f√ºr Details
  - Stelle sicher, dass LM Studio CORS erlaubt
  - Teste die URL manuell im Browser: `http://192.168.56.1:1234/v1/models`

### Aktuelles Modell nicht in der Liste
- **Ursache:** Das konfigurierte Modell ist nicht in LM Studio geladen
- **Verhalten:** Das Modell bleibt in der Konfiguration, wird aber als ung√ºltig markiert
- **L√∂sung:** W√§hle ein verf√ºgbares Modell aus der Dropdown-Liste

## Best Practices

1. **URL-Format:** Verwende `http://host:port` ohne Pfad - der Service f√ºgt automatisch den richtigen Endpoint hinzu

2. **Fallback-Modell:** Konfiguriere ein kleineres, schnelleres Modell als Fallback f√ºr den Fall, dass das prim√§re Modell nicht verf√ºgbar ist

3. **Modell-Reload:** Nach dem Laden/Entladen von Modellen in LM Studio auf üîÑ klicken, um die Liste zu aktualisieren

4. **Performance:** Schnellere Modelle (7B Parameter) sind besser f√ºr Intent-Erkennung als gro√üe Modelle (70B+)

## Zuk√ºnftige Erweiterungen

- [ ] Anzeige von Modell-Details (Gr√∂√üe, Status, Latenz)
- [ ] Automatische Modellauswahl basierend auf Performance-Metriken
- [ ] Integration mit MCP Server f√ºr Load/Unload Funktionalit√§t
- [ ] Batch-Test mehrerer Modelle gleichzeitig
- [ ] Modell-Favoriten und Presets

## Siehe auch

- [LLM_QUICKSTART.md](LLM_QUICKSTART.md) - Schnellstart f√ºr LLM-Integration
- [LLM_RUNTIME_CONFIG.md](LLM_RUNTIME_CONFIG.md) - Runtime-Konfiguration Details
- [SPEECH_VALIDATION.md](SPEECH_VALIDATION.md) - Validierungs-Pipeline

