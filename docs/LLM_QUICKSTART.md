# LLM-Validierung Schnellstart

## Was wurde ge√§ndert?

Die Sprachvalidierung nutzt jetzt **Ihr lokales Mistral-Modell** statt Code-Heuristiken.

## Vorteile

‚úÖ **Versteht nat√ºrliche Sprache** - ‚ÄûHallo und herzlich willkommen" wird akzeptiert  
‚úÖ **Toleriert STT-Fehler** - Kleine Transkriptionsfehler werden erkannt  
‚úÖ **Intelligente R√ºckfragen** - Kontext-bezogene Nachfragen statt generischer Meldungen  
‚úÖ **Kein Feintuning n√∂tig** - Prompt-basiert, sofort einsetzbar  

## Setup (einmalig)

### 1. LM Studio starten
```
1. LM Studio √∂ffnen
2. Modell "mistralai/mistral-7b-instruct-v0.3" laden
3. Local Server starten (Port 1234)
4. Pr√ºfen: http://192.168.56.1:1234 erreichbar
```

### 2. Testen
```powershell
# In PowerShell
Invoke-RestMethod -Uri "http://192.168.56.1:1234/v1/models" -Method Get
```

Sollte Ihr Modell anzeigen.

### 3. App starten
```bash
ng serve
```

## Nutzung

### Normale Verwendung

Sprechen Sie wie gewohnt:
- **Befehle**: ‚ÄûSchalte das Licht ein"
- **Begr√º√üungen**: ‚ÄûHallo und herzlich willkommen"
- **Fragen**: ‚ÄûWie sp√§t ist es?"

### LLM-Validierung l√§uft automatisch

```
User spricht ‚Üí STT ‚Üí LLM validiert ‚Üí Ergebnis
```

**Browser Console zeigt:**
```
LLM Validation Result: { isValid: true, confidence: 0.95, ... }
```

### Bei Unklarheit

LLM fragt nach:
```
‚ö†Ô∏è Was m√∂chten Sie mit dem Licht machen?
```

Dann einfach pr√§ziser sprechen.

## Troubleshooting

### LM Studio nicht erreichbar?

**Symptom:** Console zeigt
```
LLM validation failed, using simple fallback
```

**Fix:**
1. LM Studio starten
2. Local Server aktivieren
3. Port 1234 pr√ºfen

**Verhalten:** App funktioniert trotzdem (Fallback akzeptiert alle Eingaben mit reduzierter Confidence)

### Zu langsam?

**L√∂sung:** Kleineres Modell laden oder GPU aktivieren in LM Studio

### Zu viele R√ºckfragen?

**L√∂sung:** Prompt anpassen in `transcription-validator.service.ts`:
```typescript
// Zeile ~174: System-Prompt erweitern
Sei gro√üz√ºgig mit isValid=true f√ºr nat√ºrliche Sprache.
```

## Konfiguration

### Andere IP/Port

```typescript
// transcription-validator.service.ts, Zeile 30
private readonly lmStudioUrl = 'http://localhost:1234/v1/chat/completions';
```

### Anderes Modell

```typescript
// Zeile 31
private readonly model = 'meta-llama/llama-3.1-8b-instruct';
```

## Beispiele

### Vorher (Code-Heuristik)

**Eingabe:** ‚ÄûHallo und herzlich willkommen."
```
‚ùå Kein Verb erkannt
‚ö†Ô∏è Sie sagten "Hallo und herzlich willkommen". Was m√∂chten Sie damit machen?
```

### Nachher (LLM)

**Eingabe:** ‚ÄûHallo und herzlich willkommen."
```
‚úÖ Verstanden (Begr√º√üung erkannt)
‚Üí Keine Nachfrage
```

---

**Eingabe:** ‚Äûdas licht" (zu kurz)
```
‚ö†Ô∏è Was m√∂chten Sie mit dem Licht machen?
```

---

**Eingabe:** ‚Äû√§√∂√º √ü√ü√ü" (Unsinn)
```
‚ö†Ô∏è Ich konnte Sie nicht verstehen. Bitte wiederholen Sie.
```

## Performance

- **Validierung**: ~300-900ms (abh√§ngig von Hardware)
- **Fallback bei Timeout**: Eingabe wird akzeptiert
- **Keine Blockierung**: UI bleibt responsiv

## Weitere Infos

Siehe `docs/LLM_VALIDATION.md` f√ºr Details zu:
- API-Struktur
- Prompt-Engineering
- Fehlerbehandlung
- Migration
- Erweiterungen

## Status

‚úÖ **LLM-Validierung aktiv**  
‚úÖ **Fallback implementiert**  
‚úÖ **Production-ready**  

Die App versteht jetzt nat√ºrliche Sprache dank Ihrem lokalen Mistral-Modell! üéâ

